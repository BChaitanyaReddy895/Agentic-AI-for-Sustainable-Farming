#!/usr/bin/env python3
"""
Retrain ALL AI models with the large real datasets
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, accuracy_score, r2_score
import joblib
import os
from pathlib import Path

# Paths
DATASETS_DIR = Path(__file__).parent
MODELS_DIR = Path(__file__).parent.parent / 'models'

def load_all_datasets():
    """Load and combine all available datasets"""
    print("ğŸ“‚ Loading datasets...")
    
    datasets = []
    
    # 1. Load Indian crop production (246K records)
    crop_file = DATASETS_DIR / 'crop_production.csv'
    if crop_file.exists():
        df = pd.read_csv(crop_file)
        print(f"  âœ“ Indian crops: {len(df):,} records")
        datasets.append(df)
    
    # 2. Load synthetic large dataset (50K records)  
    synthetic_file = DATASETS_DIR / 'large_agricultural_dataset.csv'
    if synthetic_file.exists():
        df = pd.read_csv(synthetic_file)
        print(f"  âœ“ Synthetic data: {len(df):,} records")
        datasets.append(df)
    
    # 3. Load World Bank prices
    wb_file = DATASETS_DIR / 'world_bank_prices.csv'
    if wb_file.exists():
        df = pd.read_csv(wb_file)
        print(f"  âœ“ World Bank prices: {len(df):,} records")
        datasets.append(df)
    
    return datasets

def prepare_farmer_advisor_data(datasets):
    """Prepare data for Farmer Advisor model"""
    print("\nğŸŒ¾ Preparing Farmer Advisor data...")
    
    # Use the synthetic dataset with complete features
    df = datasets[1] if len(datasets) > 1 else datasets[0]
    
    # Features for crop recommendation
    feature_cols = ['temperature_avg', 'rainfall_mm', 'nitrogen', 'phosphorus', 'potassium', 'ph', 'organic_matter']
    target_col = 'crop'
    
    # Ensure all columns exist
    available_features = [col for col in feature_cols if col in df.columns]
    if len(available_features) < 4:
        print(f"  âš ï¸ Limited features available: {available_features}")
        return None, None, None, None
    
    X = df[available_features].fillna(df[available_features].mean())
    y = df[target_col] if target_col in df.columns else df.iloc[:, 0]
    
    print(f"  ğŸ“Š Training data: {len(X):,} samples, {len(available_features)} features")
    return train_test_split(X, y, test_size=0.2, random_state=42)

def prepare_market_data(datasets):
    """Prepare data for Market Researcher model"""
    print("\nğŸ’° Preparing Market data...")
    
    # Use synthetic agricultural dataset with price features
    df = datasets[1] if len(datasets) > 1 else datasets[0]
    
    # Market features for price prediction
    market_features = ['area_hectares', 'production_tons', 'yield_tons_per_ha', 'year', 'fertilizer_kg_ha']
    available_market = [col for col in market_features if col in df.columns]
    
    if len(available_market) < 3:
        print(f"  âš ï¸ Limited market features: {available_market}")
        return None, None, None, None
    
    # Target: price per ton
    target_col = 'price_per_ton'
    if target_col not in df.columns:
        print(f"  âš ï¸ No price column found")
        return None, None, None, None
    
    X = df[available_market].fillna(df[available_market].mean())
    y = df[target_col].fillna(df[target_col].mean())
    
    print(f"  ğŸ“Š Market data: {len(X):,} samples, {len(available_market)} features")
    return train_test_split(X, y, test_size=0.2, random_state=42)

def prepare_weather_data(datasets):
    """Prepare data for Weather Analyst model"""
    print("\nğŸŒ¤ï¸ Preparing Weather data...")
    
    # Use synthetic data with weather features
    df = datasets[1] if len(datasets) > 1 else datasets[0]
    
    weather_features = ['temperature_avg', 'rainfall_mm', 'year']
    available_weather = [col for col in weather_features if col in df.columns]
    
    if len(available_weather) < 2:
        print("  âš ï¸ Limited weather data")
        return None, None, None, None
    
    X = df[available_weather].fillna(df[available_weather].mean())
    y = df['yield_tons_per_ha'] if 'yield_tons_per_ha' in df.columns else df['production_tons']
    
    print(f"  ğŸ“Š Weather data: {len(X):,} samples, {len(available_weather)} features")
    return train_test_split(X, y, test_size=0.2, random_state=42)

def prepare_sustainability_data(datasets):
    """Prepare data for Sustainability Expert model"""
    print("\nğŸŒ± Preparing Sustainability data...")
    
    df = datasets[1] if len(datasets) > 1 else datasets[0]
    
    # Features for sustainability scoring
    sustainability_features = ['fertilizer_kg_ha', 'organic_matter', 'ph', 'nitrogen', 'phosphorus']
    available_sust = [col for col in sustainability_features if col in df.columns]
    
    if len(available_sust) < 3:
        print("  âš ï¸ Limited sustainability data")
        return None, None, None, None
    
    X = df[available_sust].fillna(df[available_sust].mean())
    
    # Create sustainability score (lower fertilizer + higher organic matter = better)
    if 'fertilizer_kg_ha' in df.columns and 'organic_matter' in df.columns:
        y = (5 - df['fertilizer_kg_ha'] / 100) + df['organic_matter']
        y = np.clip(y, 0, 10)  # Scale 0-10
    else:
        y = np.random.uniform(3, 9, len(X))  # Fallback scores
    
    print(f"  ğŸ“Š Sustainability data: {len(X):,} samples, {len(available_sust)} features")
    return train_test_split(X, y, test_size=0.2, random_state=42)

def train_farmer_advisor_model(X_train, X_test, y_train, y_test):
    """Train Farmer Advisor model"""
    print("\nğŸšœ Training Farmer Advisor Model...")
    
    # Encode crop labels
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    
    # Train Random Forest Classifier with anti-overfitting settings
    model = RandomForestClassifier(
        n_estimators=50,  # Reduced from 100
        max_depth=6,      # Reduced from 10
        min_samples_split=20,  # Require more samples to split
        min_samples_leaf=10,   # Require more samples in leaf
        max_features='sqrt',   # Use sqrt of features
        random_state=42,
        n_jobs=-1
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    model.fit(X_train_scaled, y_train_encoded)
    
    # Evaluate
    train_accuracy = model.score(X_train_scaled, y_train_encoded)
    test_accuracy = model.score(X_test_scaled, y_test_encoded)
    
    print(f"  ğŸ“Š Training Accuracy: {train_accuracy:.3f}")
    print(f"  ğŸ“Š Test Accuracy: {test_accuracy:.3f}")
    
    # Save model
    joblib.dump(model, MODELS_DIR / 'farmer_advisor_model.pkl')
    joblib.dump(scaler, MODELS_DIR / 'farmer_advisor_scaler.pkl')
    joblib.dump(le, MODELS_DIR / 'farmer_advisor_encoder.pkl')
    
    print("  âœ… Farmer Advisor model saved!")
    return model, scaler, le

def train_market_model(X_train, X_test, y_train, y_test):
    """Train Market Researcher model"""
    print("\nğŸ’¹ Training Market Model...")
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train Random Forest Regressor with anti-overfitting settings  
    model = RandomForestRegressor(
        n_estimators=50,
        max_depth=6,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    print(f"  ğŸ“Š Training RÂ²: {train_r2:.3f}")
    print(f"  ğŸ“Š Test RÂ²: {test_r2:.3f}")
    print(f"  ğŸ“Š Test RMSE: {test_rmse:.2f}")
    
    # Save model
    joblib.dump(model, MODELS_DIR / 'market_researcher_model.pkl')
    joblib.dump(scaler, MODELS_DIR / 'market_researcher_scaler.pkl')
    
    print("  âœ… Market Researcher model saved!")
    return model, scaler

def train_weather_model(X_train, X_test, y_train, y_test):
    """Train Weather Analyst model"""
    print("\nğŸŒ¦ï¸ Training Weather Model...")
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train Random Forest Regressor with anti-overfitting settings  
    model = RandomForestRegressor(
        n_estimators=50,
        max_depth=6,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    print(f"  ğŸ“Š Training RÂ²: {train_r2:.3f}")
    print(f"  ğŸ“Š Test RÂ²: {test_r2:.3f}")
    print(f"  ğŸ“Š Test RMSE: {test_rmse:.2f}")
    
    # Save model
    joblib.dump(model, MODELS_DIR / 'weather_analyst_model.pkl')
    joblib.dump(scaler, MODELS_DIR / 'weather_analyst_scaler.pkl')
    
    print("  âœ… Weather Analyst model saved!")
    return model, scaler

def train_sustainability_model(X_train, X_test, y_train, y_test):
    """Train Sustainability Expert model"""
    print("\nğŸŒ¿ Training Sustainability Model...")
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train Random Forest Regressor with anti-overfitting settings
    model = RandomForestRegressor(
        n_estimators=50,
        max_depth=6, 
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    print(f"  ğŸ“Š Training RÂ²: {train_r2:.3f}")
    print(f"  ğŸ“Š Test RÂ²: {test_r2:.3f}")
    print(f"  ğŸ“Š Test RMSE: {test_rmse:.2f}")
    
    # Save model
    joblib.dump(model, MODELS_DIR / 'sustainability_expert_model.pkl')
    joblib.dump(scaler, MODELS_DIR / 'sustainability_expert_scaler.pkl')
    
    print("  âœ… Sustainability Expert model saved!")
    return model, scaler

def main():
    print("=" * 70)
    print("ğŸ¤– RETRAINING ALL AI MODELS WITH LARGE DATASETS")
    print("=" * 70)
    
    # Create models directory
    MODELS_DIR.mkdir(exist_ok=True)
    
    # Load datasets
    datasets = load_all_datasets()
    if not datasets:
        print("âŒ No datasets found!")
        return
    
    total_records = sum(len(df) for df in datasets)
    print(f"\nğŸ“Š Total training data: {total_records:,} records")
    
    # Train each model
    trained_models = 0
    
    # 1. Farmer Advisor
    data = prepare_farmer_advisor_data(datasets)
    if data[0] is not None:
        train_farmer_advisor_model(*data)
        trained_models += 1
    
    # 2. Market Researcher
    data = prepare_market_data(datasets)
    if data[0] is not None:
        train_market_model(*data)
        trained_models += 1
    
    # 3. Weather Analyst
    data = prepare_weather_data(datasets)
    if data[0] is not None:
        train_weather_model(*data)
        trained_models += 1
    
    # 4. Sustainability Expert
    data = prepare_sustainability_data(datasets)
    if data[0] is not None:
        train_sustainability_model(*data)
        trained_models += 1
    
    print(f"\nâœ… Successfully trained {trained_models}/4 AI models!")
    print(f"ğŸ“ Models saved in: {MODELS_DIR}")
    
    # List saved models
    model_files = list(MODELS_DIR.glob('*.pkl'))
    print(f"\nğŸ“‹ Saved model files:")
    for f in sorted(model_files):
        size_mb = f.stat().st_size / (1024*1024)
        print(f"  âœ“ {f.name:<35} ({size_mb:.1f} MB)")

if __name__ == '__main__':
    main()